#layer_dropout(rate = 0.3) %>%
layer_dense(units = 200, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = 'softmax')
summary(model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 100, batch_size = 50,
validation_split = 0.25
)
plot(history)
model %>% evaluate(x_test, y_test)
?fit
?compile
## keras avec deux couches cachées sans convolution
require(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
model <- keras_model_sequential()
model %>%
layer_dense(units = 784, activation = 'relu', input_shape = c(784)) %>%
#layer_dropout(rate = 0.3) %>%
#layer_dense(units = 200, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = 'softmax')
summary(model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 10, batch_size = 200,
validation_data = list(x_test, y_test),
validation_split = 0.25
)
plot(history)
model %>% evaluate(x_test, y_test)
## keras avec deux couches cachées sans convolution
require(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
model_baseline <- keras_model_sequential()
model_baseline %>%
layer_dense(units = 784, activation = 'relu', input_shape = c(784)) %>%
#layer_dropout(rate = 0.3) %>%
#layer_dense(units = 200, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = 'softmax')
summary(model_baseline)
model_baseline %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
history_baseline <- model_baseline %>% fit(
x_train, y_train,
epochs = 10, batch_size = 200,
validation_data = list(x_test, y_test),
validation_split = 0.25
)
plot(history_baseline)
#model %>% evaluate(x_test, y_test)
# keras avec une couche de convolution
require(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
# keras avec convolution
model_conv <- keras_model_sequential()
model_conv %>%
layer_conv_2d(filters = 30, kernel_size = c(5, 5), activation = "relu", input_shape = c(28, 28, 1)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 15, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.3) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
summary(model_conv)
model_conv %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
history_conv <- model_conv %>% fit(
x_train, y_train,
epochs = 10, batch_size = 200,
validation_data = list(x_test, y_test),
validation_split = 0.25
)
plot(history_conv)
## keras avec deux couches cachées sans convolution
require(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
model_baseline <- keras_model_sequential()
model_baseline %>%
layer_dense(units = 784, activation = 'relu', input_shape = c(784)) %>%
#layer_dropout(rate = 0.3) %>%
#layer_dense(units = 200, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = 'softmax')
summary(model_baseline)
model_baseline %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
history_baseline <- model_baseline %>% fit(
x_train, y_train,
epochs = 10, batch_size = 200,
validation_data = list(x_test, y_test))
plot(history_baseline)
model_conv <- keras_model_sequential()
model_conv %>%
layer_conv_2d(filters = 30, kernel_size = c(5, 5), activation = "relu", input_shape = c(28, 28, 1)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 15, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.3) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
summary(model_conv)
# keras avec convolution
model_conv <- keras_model_sequential()
model_conv %>%
layer_conv_2d(filters = 30, kernel_size = c(5, 5), activation = "relu", input_shape = c(28, 28)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 15, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.3) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
summary(model_conv)
# keras avec convolution
rm(list=ls())
require(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
model_conv <- keras_model_sequential()
model_conv %>%
layer_conv_2d(filters = 30, kernel_size = c(5, 5), activation = "relu", input_shape = c(28, 28,1)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 15, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.3) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
summary(model_conv)
model_conv %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
history_conv <- model_conv %>% fit(
x_train, y_train,
epochs = 10, batch_size = 200,
validation_data = list(x_test, y_test),
validation_split = 0.25
)
plot(history_conv)
dim(x_train)
model_conv <- keras_model_sequential()
model_conv %>%
layer_conv_2d(filters = 30, kernel_size = c(5, 5), activation = "relu", input_shape = c(1,28, 28)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 15, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.3) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
summary(model_conv)
model_conv <- keras_model_sequential()
model_conv %>%
layer_conv_2d(filters = 30, kernel_size = c(5, 5), activation = "relu", input_shape = c(28, 28)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 15, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.3) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
summary(model_conv)
model_conv <- keras_model_sequential()
model_conv %>%
layer_conv_2d(filters = 30, kernel_size = c(5, 5), activation = "relu", input_shape = c(28, 28, 3)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 15, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.3) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
summary(model_conv)
model_conv %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
history_conv <- model_conv %>% fit(
x_train, y_train,
epochs = 10, batch_size = 200,
validation_data = list(x_test, y_test))
?layer_conv_2d
?reshape
# keras avec convolution
rm(list=ls())
require(keras)
# Input image dimensions
img_rows <- 28
img_cols <- 28
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
model_conv <- keras_model_sequential()
model_conv %>%
layer_conv_2d(filters = 30, kernel_size = c(5, 5), activation = "relu", input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 15, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.3) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
summary(model_conv)
model_conv %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
history_conv <- model_conv %>% fit(
x_train, y_train,
epochs = 10, batch_size = 200,
validation_data = list(x_test, y_test))
# keras avec convolution
rm(list=ls())
require(keras)
# Input image dimensions
img_rows <- 28
img_cols <- 28
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
model_conv <- keras_model_sequential()
model_conv %>%
layer_conv_2d(filters = 30, kernel_size = c(5, 5), activation = "relu", input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 15, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.3) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
summary(model_conv)
model_conv %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
history_conv <- model_conv %>% fit(
x_train, y_train,
epochs = 10, batch_size = 200,
validation_data = list(x_test, y_test))
rm(list=ls())
require(keras)
# Input image dimensions
img_rows <- 28
img_cols <- 28
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
model_conv <- keras_model_sequential()
model_conv %>%
layer_conv_2d(filters = 30, kernel_size = c(5, 5), activation = "relu", input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 15, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.3) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
summary(model_conv)
model_conv %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
history_conv <- model_conv %>% fit(
x_train, y_train,
epochs = 10, batch_size = 200,
validation_data = list(x_test, y_test))
?array_reshape
rm(list=ls())
require(keras)
require(dplyr)
require(caret)
#require(neuralnet)
require(doParallel)
require(kernlab)
#require(e1071)
mnist <- dataset_mnist()
x_train <- mnist$train$x
x_test  <- mnist$test$x
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
y_train <- as.factor(mnist$train$y)
y_test  <- as.factor(mnist$test$y)
rm(list=ls())
require(keras)
require(dplyr)
require(caret)
#require(neuralnet)
require(doParallel)
require(kernlab)
#require(e1071)
mnist <- dataset_mnist()
x_train <- mnist$train$x
x_test  <- mnist$test$x
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
y_train <- to_categorical(y_train, 10)
y_train <- mnist$train$y
y_test  <- mnist$test$y
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
# visualize the digits
par(mfcol=c(6,6))
par(mar=c(0, 0, 3, 0), xaxs='i', yaxs='i')
for (idx in 1:36) {
im <- x_train[idx,,]
im <- t(apply(im, 2, rev))
image(1:28, 1:28, im, col=gray((0:255)/255),
xaxt='n', main=paste(y_train[idx]))
}
# visualize the digits
par(mfcol=c(6,6))
par(mar=c(0, 0, 3, 0), xaxs='i', yaxs='i')
for (idx in 1:36) {
im <- x_train[idx,,]
im <- t(apply(im, 2, rev))
image(1:28, 1:28, im, col=gray((0:255)/255),
xaxt='n', main=paste(y_train[idx]))
}
#require(neuralnet)
require(doParallel)
require(kernlab)
#require(e1071)
mnist <- dataset_mnist()
x_train <- mnist$train$x
x_test  <- mnist$test$x
y_train <- mnist$train$y
y_test  <- mnist$test$y
# visualize the digits
par(mfcol=c(6,6))
par(mar=c(0, 0, 3, 0), xaxs='i', yaxs='i')
for (idx in 1:36) {
im <- x_train[idx,,]
im <- t(apply(im, 2, rev))
image(1:28, 1:28, im, col=gray((0:255)/255),
xaxt='n', main=paste(y_train[idx]))
}
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
## suppression et réduction de dimensions
xm <- rbind(x_train, x_test)
nzv <- nearZeroVar(xm)
xm <- xm[, -nzv]
dim(xm)
colnames(xm) <- paste("x", 1:ncol(xm), sep="")
preProcValues <- preProcess(xm, method=c("pca"))
xm_tr <- predict(preProcValues, xm)
dim(xm_tr)
xm_train_tr <- xm_tr[1:60000,]
xm_test_tr  <- xm_tr[60001:70000,]
dim(xm_train_tr)
dim(xm_test_tr)
10**(-(3:1))
10**(-(3:0))
10**(-3:0)
## keras avec deux couches cachées sans convolution
require(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
model_baseline <- keras_model_sequential()
model_baseline %>%
layer_dense(units = 784, activation = 'relu', input_shape = c(784)) %>%
layer_dense(units = 10, activation = 'softmax')
summary(model_baseline)
model_baseline %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
model_baseline %>% fit(
x_train, y_train,
epochs = 10, batch_size = 200,
validation_data = list(x_test, y_test))
?keras
